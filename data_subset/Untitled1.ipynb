{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc498a-6a7c-4e6f-bc88-ca930d3d786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import isfunction\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def uniq(arr):\n",
    "    return {el: True for el in arr}.keys()\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -torch.finfo(t.dtype).max\n",
    "\n",
    "def init_(tensor):\n",
    "    dim = tensor.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    tensor.uniform_(-std, std)\n",
    "    return tensor\n",
    "\n",
    "# 辅助模块\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class TimeShiftedMultiModalAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, max_time_lag=3, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.max_time_lag = max_time_lag\n",
    "        \n",
    "        # 可学习的滞后权重参数\n",
    "        self.lag_weights = nn.Parameter(torch.randn(max_time_lag + 1))\n",
    "        \n",
    "        # 标准QKV投影\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        \n",
    "        # 重排为多头形式 [batch, time, (heads dim)] -> [batch heads time dim]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b t (h d) -> (b h) t d', h=h), (q, k, v))\n",
    "        \n",
    "        # 计算原始注意力分数\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        \n",
    "        # 创建时间滞后索引\n",
    "        t = sim.size(1)  # 时间步数\n",
    "        rows = torch.arange(t, device=x.device).view(-1, 1)\n",
    "        cols = torch.arange(t, device=x.device).view(1, -1)\n",
    "        time_lags = (rows - cols).clamp(min=0, max=self.max_time_lag)\n",
    "        \n",
    "        # 应用滞后权重\n",
    "        lag_effect = self.lag_weights[time_lags]  # 直接索引\n",
    "        lag_effect = lag_effect.unsqueeze(0).expand(sim.size(0), -1, -1)  # 广播到batch维度\n",
    "        \n",
    "        sim = sim + lag_effect\n",
    "        \n",
    "        \n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) t d -> b t (h d)', h=h)\n",
    "        return self.to_out(out), attn.detach()\n",
    "        \n",
    "# 空间注意力（保持不变）\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "# 时间注意力（保持不变）\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, bias=None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b t (h d) -> b h t d', h=h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        if exists(bias):\n",
    "            bias = self.to_qkv(bias).chunk(3, dim=-1)\n",
    "            qb, kb, _ = map(lambda t: rearrange(t, 'b t (h d) -> b h t d', h=h), bias)\n",
    "            bias = einsum('b h i d, b h j d -> b h i j', qb, kb) * self.scale\n",
    "            dots += bias\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "# 修改后的多模态Transformer\n",
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, context_dim=9, max_time_lag=3, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, TimeShiftedMultiModalAttention(\n",
    "                    dim, \n",
    "                    context_dim=context_dim, \n",
    "                    heads=heads, \n",
    "                    dim_head=dim_head,\n",
    "                    max_time_lag=max_time_lag,\n",
    "                    dropout=dropout\n",
    "                )),\n",
    "                PreNorm(dim, FeedForward(dim, dim_out=dim, mult=mult, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        attn_weights = []\n",
    "        for attn, ff in self.layers:\n",
    "            x_out, attn = attn(x, context=context, mask=mask)\n",
    "            x = x_out + x\n",
    "            x = ff(x) + x\n",
    "            attn_weights.append(attn)\n",
    "        return self.norm(x), attn_weights[-1]  # 返回最后一层注意力\n",
    "\n",
    "# 空间Transformer（保持不变）\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, SpatialAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, dim_out=dim, mult=mult, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "# 时间Transformer（保持不变）\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, TemporalAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, dim_out=dim, mult=mult, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, bias=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, bias=bias) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7f848-2cb3-40f7-bb72-e8d2279350da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models_pvt\n",
    "from attention import TimeShiftedMultiModalAttention\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim if hidden_dim else dim * 4\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PVTSimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim=512, context_dim=9, num_head=8, \n",
    "                 mm_depth=2, dropout=0., max_time_lag=3, pretrained=True):\n",
    "        super(PVTSimCLR, self).__init__()\n",
    "        \n",
    "        self.backbone = models_pvt.__dict__[base_model](pretrained=pretrained)\n",
    "        num_ftrs = self.backbone.head.in_features\n",
    "        \n",
    "        self.proj = nn.Linear(num_ftrs, out_dim)\n",
    "        self.proj_context = nn.Linear(context_dim, out_dim)\n",
    "        self.norm1 = nn.LayerNorm(context_dim)\n",
    "        \n",
    "        dim_head = out_dim // num_head\n",
    "        self.mm_transformer = nn.ModuleList([\n",
    "            PreNorm(out_dim, TimeShiftedMultiModalAttention(\n",
    "                query_dim=out_dim,\n",
    "                context_dim=out_dim,\n",
    "                heads=num_head,\n",
    "                dim_head=dim_head,\n",
    "                max_time_lag=max_time_lag,\n",
    "                dropout=dropout\n",
    "            )) for _ in range(mm_depth)\n",
    "        ])\n",
    "        self.ff = nn.ModuleList([\n",
    "            PreNorm(out_dim, FeedForward(out_dim, dropout=dropout)) \n",
    "            for _ in range(mm_depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):  # 参数名从time_mask改为更通用的mask\n",
    "        # 视觉特征提取\n",
    "        h = self.backbone.forward_features(x)  # [B, N, D]\n",
    "        h = h.mean(dim=1) if h.dim() == 3 else h  # 确保[B, D]\n",
    "        \n",
    "        # 投影到目标维度\n",
    "        x = self.proj(h).unsqueeze(1)  # [B, 1, D]\n",
    "        context = self.proj_context(self.norm1(context))  # [B, T, D]\n",
    "        \n",
    "        # 准备mask（如果需要）\n",
    "        if mask is not None:\n",
    "            # 确保mask形状正确 [B, T]\n",
    "            if mask.dim() == 1:\n",
    "                mask = mask.unsqueeze(0).expand(x.size(0), -1)\n",
    "        \n",
    "        # 多模态时间延迟注意力\n",
    "        for attn, ff in zip(self.mm_transformer, self.ff):\n",
    "            x_attn, _ = attn(x, context=context, mask=mask)  # 传入mask\n",
    "            x = x_attn + x\n",
    "            x = ff(x) + x\n",
    "        \n",
    "        return x.squeeze(1)  # [B, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeb222-3e37-4832-8440-901c7a332e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from attention import SpatialTransformer, TemporalTransformer\n",
    "\n",
    "from models_pvt_simclr import PVTSimCLR\n",
    "\n",
    "\n",
    "class MMST_ViT(nn.Module):\n",
    "    def __init__(self, out_dim=2, num_grid=64, num_short_term_seq=6, num_long_term_seq=12, num_year=5,\n",
    "                 pvt_backbone=None, context_dim=9, dim=192, batch_size=64, depth=4, heads=3, pool='cls', dim_head=64,\n",
    "                 dropout=0., emb_dropout=0., scale_dim=4, ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pvt_backbone = pvt_backbone\n",
    "\n",
    "        self.proj_context = nn.Linear(num_year * num_long_term_seq * context_dim, num_short_term_seq * dim)\n",
    "        # self.proj_context = nn.Linear(num_year * num_long_term_seq * context_dim, dim)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_short_term_seq, num_grid, dim))\n",
    "        self.space_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.space_transformer = SpatialTransformer(dim, depth, heads, dim_head, mult=scale_dim, dropout=dropout)\n",
    "\n",
    "        self.temporal_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.temporal_transformer = TemporalTransformer(dim, depth, heads, dim_head, mult=scale_dim, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.pool = pool\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x, ys):\n",
    "        x = rearrange(x, 'b t g c h w -> (b t g) c h w')\n",
    "        ys = rearrange(ys, 'b t g n d -> (b t g) n d')\n",
    "\n",
    "        # prevent the number of grids from being too large to cause out of memory\n",
    "        B = x.shape[0]\n",
    "        n = B // self.batch_size if B % self.batch_size == 0 else B // self.batch_size + 1\n",
    "\n",
    "        x_hat = torch.empty(0).to(x.device)\n",
    "        for i in range(n):\n",
    "            start, end = i * self.batch_size, (i + 1) * self.batch_size\n",
    "            x_tmp = x[start:end]\n",
    "            ys_tmp = ys[start:end]\n",
    "\n",
    "            x_hat_tmp = self.pvt_backbone(x_tmp, context=ys_tmp)\n",
    "            x_hat = torch.cat([x_hat, x_hat_tmp], dim=0)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x, ys=None, yl=None):\n",
    "        b, t, g, _, _, _ = x.shape\n",
    "        x = self.forward_features(x, ys)\n",
    "        x = rearrange(x, '(b t g) d -> b t g d', b=b, t=t, g=g)\n",
    "\n",
    "        cls_space_tokens = repeat(self.space_token, '() g d -> b t g d', b=b, t=t)\n",
    "        x = torch.cat((cls_space_tokens, x), dim=2)\n",
    "        x += self.pos_embedding[:, :, :(g + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = rearrange(x, 'b t g d -> (b t) g d')\n",
    "        x = self.space_transformer(x)\n",
    "        x = rearrange(x[:, 0], '(b t) ... -> b t ...', b=b)\n",
    "\n",
    "        cls_temporal_tokens = repeat(self.temporal_token, '() t d -> b t d', b=b)\n",
    "        x = torch.cat((cls_temporal_tokens, x), dim=1)\n",
    "\n",
    "        # concatenate parameters in different months\n",
    "        yl = rearrange(yl, 'b y m d -> b (y m d)')\n",
    "        yl = self.proj_context(yl)\n",
    "        yl = rearrange(yl, 'b (t d) -> b t d', t=t)\n",
    "        # yl = repeat(yl, '() d -> b t d', b=b, t=t)\n",
    "\n",
    "        yl = torch.cat((cls_temporal_tokens, yl), dim=1)\n",
    "        yl = self.norm1(yl)\n",
    "\n",
    "        x = self.temporal_transformer(x, yl)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # x.shape = B, T, G, C, H, W\n",
    "    x = torch.randn((1, 6, 10, 3, 224, 224))\n",
    "    # ys.shape = B, T, G, N1, d\n",
    "    ys = torch.randn((1, 6, 10, 28, 9))\n",
    "    # yl.shape = B, T, N2, d\n",
    "    yl = torch.randn((1, 5, 12, 9))\n",
    "\n",
    "    pvt = PVTSimCLR(\"pvt_tiny\", out_dim=512, context_dim=9)\n",
    "    model = MMST_ViT(out_dim=4, pvt_backbone=pvt, dim=512)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    z = model(x, ys=ys, yl=yl)\n",
    "    print(z)\n",
    "    print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a00f46-6800-4ace-9954-13cdc3f19edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import isfunction\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def uniq(arr):\n",
    "    return {el: True for el in arr}.keys()\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -torch.finfo(t.dtype).max\n",
    "\n",
    "\n",
    "def init_(tensor):\n",
    "    dim = tensor.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    tensor.uniform_(-std, std)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# feedforward\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class MultiModalAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class TimeShiftedCrossModalAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, max_time_lag=5, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.max_time_lag = max_time_lag\n",
    "\n",
    "        # 遥感图像到查询向量\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        # 气象数据到键值向量\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        # 可学习的滞后权重参数，每个头独立学习\n",
    "        self.lag_weights = nn.Parameter(torch.randn(heads, max_time_lag + 1))\n",
    "        \n",
    "        # 输出层\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        h = self.heads\n",
    "\n",
    "        # 查询来自遥感图像\n",
    "        q = self.to_q(x)\n",
    "        \n",
    "        # 键值来自气象数据\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        # 重排为多头形式\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b t (h d) -> (b h) t d', h=h), (q, k, v))\n",
    "\n",
    "        # 计算原始注意力分数\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        # 创建因果掩码 - 遥感图像只能访问当前及之前时刻的气象数据\n",
    "        t = sim.size(1)\n",
    "        causal_mask = torch.ones(t, t, device=x.device).triu_(1).bool()\n",
    "        max_neg_val = -torch.finfo(sim.dtype).max\n",
    "        sim.masked_fill_(causal_mask.unsqueeze(0), max_neg_val)\n",
    "\n",
    "        # 应用时间滞后权重\n",
    "        time_lags = torch.arange(t, device=x.device).view(1, -1, 1) - torch.arange(t, device=x.device).view(1, 1, -1)\n",
    "        time_lags = time_lags.clamp(min=0, max=self.max_time_lag)\n",
    "        \n",
    "        # 扩展滞后权重到batch维度\n",
    "        lag_effect = self.lag_weights.unsqueeze(0).unsqueeze(3)  # [1, heads, max_lag+1, 1]\n",
    "        lag_effect = lag_effect.expand(sim.size(0), -1, -1, t)   # [batch*heads, heads, max_lag+1, t]\n",
    "        \n",
    "        # 为每个头选择对应的滞后权重\n",
    "        head_indices = torch.arange(h, device=x.device).repeat(sim.size(0) // h)\n",
    "        selected_lag_weights = lag_effect[torch.arange(sim.size(0)), head_indices]  # [batch*heads, max_lag+1, t]\n",
    "        \n",
    "        # 应用滞后效应\n",
    "        lag_adjustment = selected_lag_weights.gather(1, time_lags.expand(sim.size(0), -1, -1))\n",
    "        sim = sim + lag_adjustment\n",
    "\n",
    "        # 注意力计算\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) t d -> b t (h d)', h=h)\n",
    "        \n",
    "        # 返回输出张量\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, context_dim=9, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, MultiModalAttention(dim, context_dim=context_dim, heads=heads, dim_head=dim_head,\n",
    "                                                 dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, dim_out=dim, mult=mult, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context=context) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, SpatialAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, dim_out=dim, mult=mult, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, max_time_lag=5, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, TimeShiftedCrossModalAttention(\n",
    "                    query_dim=dim, \n",
    "                    context_dim=dim, \n",
    "                    heads=heads, \n",
    "                    dim_head=dim_head, \n",
    "                    max_time_lag=max_time_lag, \n",
    "                    dropout=dropout\n",
    "                )),\n",
    "                PreNorm(dim, FeedForward(dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, bias=None, context=None):\n",
    "        for attn, ff in self.layers:\n",
    "            if context is not None:\n",
    "                # 使用跨模态注意力，x作为query，context作为key/value\n",
    "                x = attn(x, context=context) + x\n",
    "            else:\n",
    "                # 使用自注意力\n",
    "                x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ca79e-4893-48e0-8597-4ff1cfeb8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from attention import SpatialTransformer, TemporalTransformer\n",
    "\n",
    "from models_pvt_simclr import PVTSimCLR\n",
    "\n",
    "\n",
    "class MMST_ViT(nn.Module):\n",
    "    def __init__(self, out_dim=2, num_grid=64, num_short_term_seq=6, num_long_term_seq=12, num_year=5,\n",
    "                 pvt_backbone=None, context_dim=9, dim=192, batch_size=64, depth=4, heads=3, pool='cls', dim_head=64,\n",
    "                 dropout=0., emb_dropout=0., scale_dim=4, max_time_lag=5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pvt_backbone = pvt_backbone\n",
    "\n",
    "        self.proj_context = nn.Linear(num_year * num_long_term_seq * context_dim, num_short_term_seq * dim)\n",
    "        # self.proj_context = nn.Linear(num_year * num_long_term_seq * context_dim, dim)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_short_term_seq, num_grid, dim))\n",
    "        self.space_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.space_transformer = SpatialTransformer(dim, depth, heads, dim_head, mult=scale_dim, dropout=dropout)\n",
    "\n",
    "        self.temporal_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        # 更新 TemporalTransformer 初始化，添加 max_time_lag 参数\n",
    "        self.temporal_transformer = TemporalTransformer(\n",
    "            dim=dim, \n",
    "            depth=depth, \n",
    "            heads=heads, \n",
    "            dim_head=dim_head, \n",
    "            max_time_lag=max_time_lag,  # 添加时间滞后参数\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.pool = pool\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x, ys):\n",
    "        x = rearrange(x, 'b t g c h w -> (b t g) c h w')\n",
    "        ys = rearrange(ys, 'b t g n d -> (b t g) n d')\n",
    "\n",
    "        # prevent the number of grids from being too large to cause out of memory\n",
    "        B = x.shape[0]\n",
    "        n = B // self.batch_size if B % self.batch_size == 0 else B // self.batch_size + 1\n",
    "\n",
    "        x_hat = torch.empty(0).to(x.device)\n",
    "        for i in range(n):\n",
    "            start, end = i * self.batch_size, (i + 1) * self.batch_size\n",
    "            x_tmp = x[start:end]\n",
    "            ys_tmp = ys[start:end]\n",
    "\n",
    "            x_hat_tmp = self.pvt_backbone(x_tmp, context=ys_tmp)\n",
    "            x_hat = torch.cat([x_hat, x_hat_tmp], dim=0)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x, ys=None, yl=None):\n",
    "        b, t, g, _, _, _ = x.shape\n",
    "        x = self.forward_features(x, ys)\n",
    "        x = rearrange(x, '(b t g) d -> b t g d', b=b, t=t, g=g)\n",
    "\n",
    "        cls_space_tokens = repeat(self.space_token, '() g d -> b t g d', b=b, t=t)\n",
    "        x = torch.cat((cls_space_tokens, x), dim=2)\n",
    "        x += self.pos_embedding[:, :, :(g + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = rearrange(x, 'b t g d -> (b t) g d')\n",
    "        x = self.space_transformer(x)\n",
    "        x = rearrange(x[:, 0], '(b t) ... -> b t ...', b=b)\n",
    "\n",
    "        cls_temporal_tokens = repeat(self.temporal_token, '() t d -> b t d', b=b)\n",
    "        x = torch.cat((cls_temporal_tokens, x), dim=1)\n",
    "\n",
    "        # concatenate parameters in different months\n",
    "        yl = rearrange(yl, 'b y m d -> b (y m d)')\n",
    "        yl = self.proj_context(yl)\n",
    "        yl = rearrange(yl, 'b (t d) -> b t d', t=t)\n",
    "        # yl = repeat(yl, '() d -> b t d', b=b, t=t)\n",
    "\n",
    "        yl = torch.cat((cls_temporal_tokens, yl), dim=1)\n",
    "        yl = self.norm1(yl)\n",
    "\n",
    "        # 更新 TemporalTransformer 调用，传递气象数据作为上下文\n",
    "        x = self.temporal_transformer(x, context=yl)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # x.shape = B, T, G, C, H, W\n",
    "    x = torch.randn((1, 6, 10, 3, 224, 224))\n",
    "    # ys.shape = B, T, G, N1, d\n",
    "    ys = torch.randn((1, 6, 10, 28, 9))\n",
    "    # yl.shape = B, T, N2, d\n",
    "    yl = torch.randn((1, 5, 12, 9))\n",
    "\n",
    "    pvt = PVTSimCLR(\"pvt_tiny\", out_dim=512, context_dim=9)\n",
    "    model = MMST_ViT(out_dim=4, pvt_backbone=pvt, dim=512, max_time_lag=5)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    z = model(x, ys=ys, yl=yl)\n",
    "    print(z)\n",
    "    print(z.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
